{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS_Stations Class\n",
    "The `GPS_Stations` class is a wrapper for a data dictionary, `{Name: time series data}`. The time series data is an Nx4 ndarray, where N is the number of dates, from 2011/4/1 to 2016/12/31. The 4 columns correspond to date (`datetime` type), east, north and up components (all `float` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPS_Stations():\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "    def add(self, station_name, date, x, y, z):\n",
    "        if station_name in self.data:\n",
    "            # if the station is already in the data dictionary, append a new row of data [date,x,y,z]\n",
    "            self.data.update( {station_name : np.append( self.data[station_name], np.array([date,x,y,z]).reshape(1,4), axis=0 ) } )\n",
    "        else: # if first time adding this station, create a key:data pair.\n",
    "            self.data.update( {station_name : np.array([date,x,y,z]).reshape(1,4)} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "If a pickled data object exists, load and use it as an instance of `GPS_Stations` class named `Korea_GPS_Stations`. If not, i.e., `DATA_EXISTS==False`, read the data files retrived from the repository associated with (Kim et al., Tectonics, 2018) to create and populate an instance of `GPS_Stations` named `Korea_GPS_Stations`. \n",
    "\n",
    "If a pickled `GPS_Stations` data object exists, the first cell will just load it rather than read the coordinate files again. In this case, `DATA_EXISTS` is set to `True` so that the second cell can be skipped.\n",
    "\n",
    "If such a pickled object does not exist (`DATA_EXISTS == False`), the second cell below should be run. Then, the newly created object is pickled for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_EXISTS = False\n",
    "try: \n",
    "    Korea_GPS_Stations = pickle.load( open( \"Korea_GPS_Stations.p\", \"rb\" ) )\n",
    "    if Korea_GPS_Stations:\n",
    "        DATA_EXISTS = True\n",
    "        print(\"Data object exists\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Failed to load the pickled data. Move to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_EXISTS == False:\n",
    "    data_dir = \"GPS Daily Coordinate data (2011 to 2016)/Coordinate\"\n",
    "    Korea_GPS_Stations = GPS_Stations()\n",
    "    years = np.arange(2011,2017)\n",
    "    #print(years)\n",
    "    networks = np.array(['KASI', 'NGII'])\n",
    "    #print(networks)\n",
    "    for yr in years:\n",
    "        for nt in networks:\n",
    "            path = \"./{0:s}/{1}/{2}\".format(data_dir, yr, nt)\n",
    "            #print(path)\n",
    "            for f in os.listdir(path):\n",
    "                fname = \"{0}/{1}\".format(path,f)\n",
    "                fi = open(fname, 'r')\n",
    "                lines = fi.readlines()\n",
    "                date = np.datetime64(lines[2].split()[5]).astype(datetime)\n",
    "                if date == datetime(2012,6,1): # exclude outlier\n",
    "                    continue\n",
    "                linecount = 7\n",
    "                for line in lines[7:]:\n",
    "                    linecount += 1\n",
    "                    fields = line.split()\n",
    "                    fieldnum = len(fields)\n",
    "                    if fieldnum == 6:   # valid data without 'Name' field\n",
    "                        name = fields[1]\n",
    "                        x = float(fields[2])\n",
    "                        y = float(fields[3])\n",
    "                        z = float(fields[4])\n",
    "                    elif fieldnum == 7: # valid data with all the fields filled\n",
    "                        name = fields[1]\n",
    "                        x = float(fields[3])\n",
    "                        y = float(fields[4])\n",
    "                        z = float(fields[5])\n",
    "                    elif fieldnum == 5:   # invalid data with 0 0 0 coordinates\n",
    "                        continue  # move on to the next line\n",
    "                    elif fieldnum <= 1: # bottom blank lines\n",
    "                        break     # stop processing this file\n",
    "                    else:               # None of the above. Abort\n",
    "                        print(\"Problem with line {0} in {1}\".format(linecount, fname))\n",
    "                        break     # stop processing this file\n",
    "                    #print(fname, line)\n",
    "                    Korea_GPS_Stations.add(name, date, x, y, z)\n",
    "                fi.close()\n",
    "\n",
    "    # Store the data object for quicker loading later\n",
    "    pickle.dump(Korea_GPS_Stations, open( \"Korea_GPS_Stations.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jump removal\n",
    "A function `remove_jump` is used for removing a jump in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_jump( stations, s, jump_date1, jump_date2 ):\n",
    "    d = stations.data[s][:,0]\n",
    "    e = stations.data[s][:,1]\n",
    "    n = stations.data[s][:,2]\n",
    "    u = stations.data[s][:,3]\n",
    "\n",
    "    jump_ind1 = np.where(d == jump_date1)[0]\n",
    "    jump_ind2 = np.where(d == jump_date2)[0]\n",
    "    after_jump = d > jump_date1\n",
    "    \n",
    "    #print(jump_ind1, jump_ind2)\n",
    "    #print(jump_ind2[0], jump_ind2[0]+5, stations.data[s][jump_ind2[0]:(jump_ind2[0]+5),1:4])\n",
    "    avg_period = 20\n",
    "    if s == 'JEOJ':\n",
    "        avg_period = 120\n",
    "    jumpval = np.sum(stations.data[s][jump_ind2[0]:(jump_ind2[0]+avg_period),1:4],\\\n",
    "                     axis=0)/avg_period\n",
    "    print(jumpval)\n",
    "    jump = jumpval - stations.data[s][jump_ind1,1:4]\n",
    "    #jump = stations.data[s][jump_ind2,1:4]-stations.data[s][jump_ind1,1:4]\n",
    "    #print(jump.shape)\n",
    "    new_e = np.where(after_jump, e-jump[0,0], e)\n",
    "    new_n = np.where(after_jump, n-jump[0,1], n)\n",
    "    new_u = np.where(after_jump, u-jump[0,2], u)\n",
    "    #plt.plot(d, new_u, color='red')\n",
    "    #plt.plot(d, u)\n",
    "\n",
    "    stations.data[s][:,1] = new_e\n",
    "    stations.data[s][:,2] = new_n\n",
    "    stations.data[s][:,3] = new_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove jump in some stations\n",
    "remove_jump( Korea_GPS_Stations, 'HOMI', datetime(2013,8,4).date(), datetime(2013,8,5).date())\n",
    "remove_jump( Korea_GPS_Stations, 'JEOJ', datetime(2013,8,1).date(), datetime(2013,8,3).date())\n",
    "remove_jump( Korea_GPS_Stations, 'MARA', datetime(2012,9,19).date(), datetime(2012,9,20).date())\n",
    "remove_jump( Korea_GPS_Stations, 'ULLE', datetime(2014,9,16).date(), datetime(2014,9,17).date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stations with incomplete time period\n",
    "\n",
    "Most of the stations have a position time series spanning 2011/04/01 to 2016/12/31.\n",
    "Any station with a shorter data span is removed from the `Korea_GPS_Stations object`'s data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the loaded data if desired.\n",
    "stations_to_discard = []\n",
    "for s in Korea_GPS_Stations.data.keys():\n",
    "    if Korea_GPS_Stations.data[s][0,0] > datetime(2011,4,1).date() or \\\n",
    "        Korea_GPS_Stations.data[s][-1,0] < datetime(2016,12,31).date():\n",
    "        stations_to_discard.append(s)\n",
    "for s in stations_to_discard:\n",
    "    Korea_GPS_Stations.data.pop(s)\n",
    "    \n",
    "scounter = 0\n",
    "for s in Korea_GPS_Stations.data.keys():    \n",
    "    scounter += 1\n",
    "    print(scounter, s, Korea_GPS_Stations.data[s][0,0], Korea_GPS_Stations.data[s][-1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers with Extended Linear Trajectory Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_station_data()`\n",
    "A function that returns date, easting, northing and up for a `station` from `start_datetime` to `end_datetime`.\n",
    "\n",
    "#### `plot_selected_station()`\n",
    "A function that plots easting, northing and up for a `selected_station` from `start_datetime` to `end_datetime`.\n",
    "\n",
    "#### `eltm()` and `eltm2()`\n",
    "Implementation of Extended Linear Trajectory Model (Bevis and Brown, J.Geodesy, 2014):\n",
    "\n",
    "$\n",
    "\\mathbf{x}(t) = \\sum_{i=1}^{np+1} \\mathbf{p}_{i}(t-t_{R})^{i-1} +\n",
    "\\sum_{j=1}^{nJ} \\mathbf{b}_{j}H(t-t_{j}) + \n",
    "\\sum_{k=1}^{nF} \\left( \\mathbf{s}_{k}\\sin(\\omega_{k}t) + \\mathbf{c}_{k}\\cos(\\omega_{k}t) \\right) +\n",
    "\\sum_{m=1}^{nT} \\mathbf{a}_{m}\\log(1+\\Delta t_{m}/T_{m}).\n",
    "$\n",
    "\n",
    "\n",
    "The difference between `eltm` and `eltm2` is that `eltm` takes `days` as `float` while `eltm2` converts `datetime` to `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_data( station, start_datetime=datetime(2011, 4, 1), end_datetime=datetime(2016, 12, 31) ):\n",
    "    \"\"\"\n",
    "    Retrieves a station's data from start_datetime to end_datetime (all inclusive)\n",
    "    and returns 4 arrays of daily time series, date, easting, northing and up.\n",
    "    Note that position components are shifted to be zero on the starting date.\n",
    "    \"\"\"\n",
    "    data = Korea_GPS_Stations.data[station]\n",
    "    start_date = start_datetime.date() \n",
    "    if start_date < data[0,0]:\n",
    "        start_date = data[0,0]\n",
    "        print(\"Warning: Start date reset to {0}\".format(start_date))\n",
    "    end_date = end_datetime.date()\n",
    "    if end_date > data[-1,0]:\n",
    "        end_date = data[-1,0]\n",
    "        print(\"Warning: End date reset to {0}\".format(end_date))\n",
    "    #print(start_date, data[0,0])\n",
    "    \n",
    "    datemask = np.logical_and(data[:,0] >= start_date, data[:,0] <= end_date)\n",
    "    start_index = np.where(data[:,0] == start_date)[0]\n",
    "    if len(start_index) != 1:\n",
    "        print(start_index)\n",
    "        print(data[0,0], data[-1,0])\n",
    "    \n",
    "    date = data[datemask,0]\n",
    "    easting  = data[start_index,1] - data[datemask,   1]\n",
    "    northing = data[datemask,   2] - data[start_index,2]\n",
    "    up       = data[datemask,   3] - data[start_index,3]\n",
    "    #print(data[start_index,2], data[start_index,3])\n",
    "    #print(len(date), len(data[:,0]))\n",
    "    \n",
    "    return date, easting, northing, up\n",
    "\n",
    "def plot_selected_station( selected_station, start_datetime=datetime(2011, 9, 1), \n",
    "                          end_datetime=datetime(2016, 9, 30) ):\n",
    "    \"\"\"\n",
    "    plot the east, north and up position time series for a selected station \n",
    "    from a start date until an end date.\n",
    "    \"\"\"\n",
    "    d, e, n, u = get_station_data(selected_station, start_datetime, end_datetime)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    ax.plot(d, e, 'ro', markersize=2, label=\"east\")\n",
    "    ax.plot(d, n, 'go', markersize=2, label=\"north\")\n",
    "    ax.plot(d, u, 'bo', markersize=2, label=\"up\")\n",
    "    ax.set_title(selected_station)\n",
    "    ax.set_ylabel('Displacement (m)')\n",
    "    ax.set_ylim((-0.1,0.1))\n",
    "    ax.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eltm(days, p, q, sa, sp, ca, cp, sh, shp, ch, chp, a):\n",
    "    from numpy import sin, cos, log   \n",
    "    wa = 2.0*np.pi/365.0  # annual frequency\n",
    "    wh = 4.0*np.pi/365.0  # semi-annual frequency\n",
    "    # Number of days since Tohoku Eq (March 11, 2011) until April 1, 2011.\n",
    "    dd_eq = (datetime(2011, 4, 1)-datetime(2011, 3, 11)) / timedelta(days=1)\n",
    "    dd = 2.0*(days + dd_eq)/365.0 # time scale: half a year\n",
    "    disp = (p*days+q) + sa * sin(wa*(days-sp)) + ca * cos(wa*(days-cp)) \\\n",
    "            + sh * sin(wh*(days-shp)) + ch * cos(wh*(days-chp)) + a * log(1.0+dd)\n",
    "    return disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eltm2(day, coeff):\n",
    "    days = day.astype(float)\n",
    "    p, q, sa, sp, ca, cp, sh, shp, ch, chp, a = coeff\n",
    "    from numpy import sin, cos, log   \n",
    "    wa = 2.0*np.pi/365.0  # annual frequency\n",
    "    wh = 4.0*np.pi/365.0  # semi-annual frequency\n",
    "    # Number of days since Tohoku Eq (March 11, 2011) until April 1, 2011.\n",
    "    dd_eq = (datetime(2011, 4, 1)-datetime(2011, 3, 11)) / timedelta(days=1)\n",
    "    dd = 2.0*(days + dd_eq)/365.0 # time scale: half a year\n",
    "    disp = (p*days+q) + sa * sin(wa*(days-sp)) + ca * cos(wa*(days-cp)) \\\n",
    "            + sh * sin(wh*(days-shp)) + ch * cos(wh*(days-chp)) + a * log(1.0+dd)\n",
    "    return disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_eltm( selected_station, d, e, n, u, e_eltm, cleaned_station_data, popt ):\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    for ax in axs:\n",
    "        for axis in ['top', 'bottom', 'left', 'right']:\n",
    "            ax.spines[axis].set_linewidth(2.0)\n",
    "    fig.set_figheight(4)\n",
    "    fig.set_figwidth(9)\n",
    "\n",
    "    axs[0].plot(d,e)\n",
    "    axs[0].tick_params(direction='out', length=6, width=2, labelsize=12)\n",
    "    axs[0].tick_params(which='minor', direction='out', length=4, width=2)\n",
    "    axs[0].set_title(\"{0}: Raw data\".format(selected_station))\n",
    "    axs[0].set_xlabel('Time', size=14)\n",
    "    axs[0].set_ylabel('Easting (m)', size=14)\n",
    "    axs[0].grid()\n",
    "\n",
    "    # Plot both data and the fitting function\n",
    "    from numpy import log\n",
    "    axs[1].plot(cleaned_station_data['days'], cleaned_station_data['e'],\\\n",
    "                marker='.', markeredgecolor='gray', markerfacecolor='gray', linestyle='none', alpha=1.0, label='Cleaned data')\n",
    "    axs[1].set_ylim((-0.05,0.2))\n",
    "    axs[1].plot(cleaned_station_data['days'], e_eltm, color='tab:red',\\\n",
    "            linewidth=2, label='ELTM')\n",
    "    axs[1].plot(cleaned_station_data['days'], popt[0]*cleaned_station_data['dfloat'] + popt[1], \\\n",
    "             color='tab:orange', linewidth=2, label='linear')\n",
    "    axs[1].plot(cleaned_station_data['days'], popt[0]*cleaned_station_data['dfloat'] + popt[1] \\\n",
    "             + popt[10]*log(1.0+2.0*(cleaned_station_data['dfloat'] + 20.0)/365.0),\\\n",
    "            color='tab:green', linewidth=2, label='linear+log')\n",
    "    axs[1].tick_params(direction='out', length=6, width=2, labelsize=12)\n",
    "    axs[1].tick_params(which='minor', direction='out', length=4, width=2)\n",
    "    axs[1].set_title(\"{0}: Cleaned data and ELTM\".format(selected_station))\n",
    "    axs[1].set_xlabel('Time', size=14)\n",
    "    axs[1].set_ylabel('Easting (m)', size=14)\n",
    "    legend = axs[1].legend(loc=4, fontsize=12)\n",
    "    legend.get_frame().set_linewidth(2)\n",
    "    axs[1].grid()\n",
    "\n",
    "    plt.savefig(\"./ELTM_plots/Cleaned_ELTM_{0}.pdf\".format(selected_station))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eltm_and_plot( selected_station ):    \n",
    "    d, e, n, u = get_station_data(selected_station, datetime(2011, 4, 1), \\\n",
    "                                  end_datetime=datetime(2016, 12, 31))\n",
    "\n",
    "    from scipy.optimize import curve_fit\n",
    "    days_float = (d-datetime(2011, 4, 1).date()) / timedelta(days=1)\n",
    "    popt, pcov = curve_fit(eltm, days_float, e)\n",
    "    e_eltm = eltm2(days_float, popt)\n",
    "    e_detrended = e - e_eltm\n",
    "\n",
    "    # Outlier detect/removal\n",
    "    std_e = np.std(e_detrended)\n",
    "    max_deviations = 1.0\n",
    "    not_outlier = np.abs(e_detrended) < (max_deviations * std_e)\n",
    "\n",
    "    cleaned_station_data = {}\n",
    "    cleaned_station_data['days'] = d[not_outlier]\n",
    "    cleaned_station_data['e'] = e[not_outlier]\n",
    "    cleaned_station_data['n'] = n[not_outlier]\n",
    "    cleaned_station_data['u'] = u[not_outlier]\n",
    "    cleaned_station_data['dfloat'] = days_float[not_outlier].astype('float64')\n",
    "\n",
    "    # Acquire ELTM once again with the cleaned data\n",
    "    popt, pcov = curve_fit(eltm, cleaned_station_data['dfloat'], cleaned_station_data['e'])\n",
    "    e_eltm = eltm2(cleaned_station_data['dfloat'], popt)\n",
    "    \n",
    "    plot_raw_eltm( selected_station, d, e, n, u, e_eltm, cleaned_station_data, popt)\n",
    "    \n",
    "    return popt, pcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station selector widget\n",
    "station_selector = widgets.Select(\n",
    "    options=Korea_GPS_Stations.data.keys(),\n",
    "    # NOTE (rclam): seems this has to be manually changed on my end when change GPS station\n",
    "    value='DAEJ', \n",
    "    rows=10,\n",
    "    description='Select a GPS station:',\n",
    "    disabled=False\n",
    ")\n",
    "display(station_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When processing stations one by one:\n",
    "selected_station = station_selector.value\n",
    "popt, pcov = get_eltm_and_plot( selected_station )\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data > 1 $\\sigma$ from the detrended filtered out.\n",
    "- Notes on 22 problematic stations\n",
    "    - 3 stations are retained after jump removed:\n",
    "        - HOMI, MARA, ULLE\n",
    "    - 19 stations discarded\n",
    "        - WPWN: Station unidentified and location unknown\n",
    "        - JEOJ: Jump removed unsatisfactorily.\n",
    "        - SOUL: Data corrupted after early 2015?\n",
    "        - TABK: Data corrupted after early 2012?\n",
    "        - DOKD: Data corrupted?\n",
    "        - SEJO: Data length shorter than others.\n",
    "        - GOJE: Data length shorter than others.\n",
    "        - KUSN: Data length shorter than others.\n",
    "        - YODK: Data length shorter than others.\n",
    "        - JIND: Data length shorter than others.\n",
    "        - YOIN: Data length shorter than others.\n",
    "        - SEJN: Data length shorter than others.\n",
    "        - GANH: Data length shorter than others.\n",
    "        - BONH: Data length shorter than others.\n",
    "        - DONH: Data length shorter than others.\n",
    "        - CHUL: Data length shorter than others.\n",
    "        - DANJ: Data length shorter than others.\n",
    "        - GOSG: Data length shorter than others.\n",
    "        - HCHN: Data length shorter than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Korea_GPS_Stations.data.keys()))\n",
    "problematic_stations = np.array(['JEOJ','SOUL','TABK','DOKD','SEJO','GOJE',\\\n",
    "                                 'KUSN','YODK','JIND','YOIN','SEJN','GANH',\\\n",
    "                                 'BONH','DONH','CHUL','DANJ','GOSG','HCHN','WPWN'])\n",
    "for station in problematic_stations:\n",
    "    try:\n",
    "        Korea_GPS_Stations.data.pop(station)\n",
    "        print(\"{0} removed\".format(station))\n",
    "    except KeyError:\n",
    "        print(\"{0} already removed\".format(station))\n",
    "        continue\n",
    "print(len(Korea_GPS_Stations.data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When processing all the stations in a batch:\n",
    "counter = 0\n",
    "station_eltm_data = {}\n",
    "for selected_station in Korea_GPS_Stations.data.keys():\n",
    "    popt, pcov = get_eltm_and_plot( selected_station )\n",
    "    counter = counter + 1\n",
    "    station_eltm_data[selected_station] = {'coeffs':popt, 'cov':pcov}\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set with the problematic stations removed is pickled as `Korea_GPS_Stations_20110401_20161231.p`.\n",
    "The linear and logarithmic coefficients and associated covariances from the ELTM fitting are saved as `Korea_GPS_Stations_coeff_cov_20110401_20161231.p`. Subsequent processing and analysis can start with loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stations that has data from 2011/04/11 to 2016/09/11.\n",
    "pickle.dump(Korea_GPS_Stations, open( \"Korea_GPS_Stations_20110401_20161231.p\", \"wb\" ) )    \n",
    "pickle.dump(Korea_GPS_Stations, open( \"Korea_GPS_Stations_coeff_cov_20110401_20161231.p\", \"wb\" ) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Map of the GPS Stations\n",
    "\n",
    "The locations of the GPS stations are plotted for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "station_locations = pd.read_excel (r'GPS_station_locations.xlsx')\n",
    "#print (station_locations['Station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.io.img_tiles as cimgt\n",
    "stamen_terrain = cimgt.Stamen('terrain-background')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(8)\n",
    "\n",
    "#ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax = fig.add_subplot(1, 1, 1, projection=stamen_terrain.crs)\n",
    "ax.set_extent([125, 131, 33, 39], crs=ccrs.PlateCarree())\n",
    "\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# Add the Stamen data at zoom level 8.\n",
    "ax.add_image(stamen_terrain, 8)\n",
    "ax.coastlines(resolution='10m')\n",
    "gl = ax.gridlines(draw_labels=True, color='white')\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_right = False\n",
    "gl.xlocator = mticker.FixedLocator([125, 126, 127, 128, 129, 130, 131])\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.ylabel_style = {'size': 17, 'color': 'black'}\n",
    "gl.xlabel_style = {'size': 17, 'color': 'black'}\n",
    "\n",
    "counter = 0\n",
    "print(len(Korea_GPS_Stations.data.keys()))\n",
    "for station in Korea_GPS_Stations.data.keys():\n",
    "    loc_ind = station_locations.index[station_locations['Station'] == station].to_list()\n",
    "    if len(loc_ind) == 1:\n",
    "        counter = counter + 1\n",
    "        #print(\"{0} Found index for {1}: {2}\".format(counter, station, loc_ind[0]))        \n",
    "    else:\n",
    "        print(\"Location not found: {0}\".format(station))\n",
    "        continue\n",
    "    lon = station_locations.at[loc_ind[0], 'Longitude (deg)']\n",
    "    lat = station_locations.at[loc_ind[0], 'Latitude (deg)']\n",
    "    ax.plot(lon, lat, marker='o', color='red', markersize=12,\n",
    "            alpha=0.7, transform=ccrs.PlateCarree())\n",
    "    ax.text(lon, lat+0.1, station,\n",
    "            verticalalignment='center', horizontalalignment='right',\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            bbox=dict(facecolor='sandybrown', alpha=0.5, boxstyle='round'))\n",
    "print(counter)\n",
    "plt.show()\n",
    "fig.savefig(\"GPS_location_map.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
