{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS_Stations Class\n",
    "The `GPS_Stations` class is a wrapper for a data dictionary, `{Name: time series data}`. The time series data is an Nx4 ndarray, where N is the number of dates, from 2011/4/1 to 2016/12/31. The 4 columns correspond to date (`datetime` type), east, north and up components (all `float` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPS_Stations():\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "    def add(self, station_name, date, x, y, z):\n",
    "        if station_name in self.data:\n",
    "            # if the station is already in the data dictionary, append a new row of data [date,x,y,z]\n",
    "            self.data.update( {station_name : np.append( self.data[station_name], np.array([date,x,y,z]).reshape(1,4), axis=0 ) } )\n",
    "        else: # if first time adding this station, create a key:data pair.\n",
    "            self.data.update( {station_name : np.array([date,x,y,z]).reshape(1,4)} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "If a pickled data object exists, load and use it as an instance of `GPS_Stations` class named `Korea_GPS_Stations`. If not, i.e., `DATA_EXISTS==False`, read the data files retrived from the repository associated with (Kim et al., Tectonics, 2018) to create and populate an instance of `GPS_Stations` named `Korea_GPS_Stations`. \n",
    "\n",
    "If a pickled `GPS_Stations` data object exists, the first cell will just load it rather than read the coordinate files again. In this case, `DATA_EXISTS` is set to `True` so that the second cell can be skipped.\n",
    "\n",
    "If such a pickled object does not exist (`DATA_EXISTS == False`), the second cell below should be run. Then, the newly created object is pickled for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_EXISTS = False\n",
    "try: \n",
    "    Korea_GPS_Stations = pickle.load( open( \"Korea_GPS_Stations.p\", \"rb\" ) )\n",
    "    if Korea_GPS_Stations:\n",
    "        DATA_EXISTS = True\n",
    "        print(\"Data object exists\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Failed to load the pickled data. Move to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_EXISTS == False:\n",
    "    data_dir = \"GPS Daily Coordinate data (2011 to 2016)/Coordinate\"\n",
    "    Korea_GPS_Stations = GPS_Stations()\n",
    "    years = np.arange(2011,2017)\n",
    "    #print(years)\n",
    "    networks = np.array(['KASI', 'NGII'])\n",
    "    #print(networks)\n",
    "    for yr in years:\n",
    "        for nt in networks:\n",
    "            path = \"./{0:s}/{1}/{2}\".format(data_dir, yr, nt)\n",
    "            #print(path)\n",
    "            for f in os.listdir(path):\n",
    "                fname = \"{0}/{1}\".format(path,f)\n",
    "                fi = open(fname, 'r')\n",
    "                lines = fi.readlines()\n",
    "                date = np.datetime64(lines[2].split()[5]).astype(datetime)\n",
    "                if date == datetime(2012,6,1): # exclude outlier\n",
    "                    continue\n",
    "                linecount = 7\n",
    "                for line in lines[7:]:\n",
    "                    linecount += 1\n",
    "                    fields = line.split()\n",
    "                    fieldnum = len(fields)\n",
    "                    if fieldnum == 6:   # valid data without 'Name' field\n",
    "                        name = fields[1]\n",
    "                        x = float(fields[2])\n",
    "                        y = float(fields[3])\n",
    "                        z = float(fields[4])\n",
    "                    elif fieldnum == 7: # valid data with all the fields filled\n",
    "                        name = fields[1]\n",
    "                        x = float(fields[3])\n",
    "                        y = float(fields[4])\n",
    "                        z = float(fields[5])\n",
    "                    elif fieldnum == 5:   # invalid data with 0 0 0 coordinates\n",
    "                        continue  # move on to the next line\n",
    "                    elif fieldnum <= 1: # bottom blank lines\n",
    "                        break     # stop processing this file\n",
    "                    else:               # None of the above. Abort\n",
    "                        print(\"Problem with line {0} in {1}\".format(linecount, fname))\n",
    "                        break     # stop processing this file\n",
    "                    #print(fname, line)\n",
    "                    Korea_GPS_Stations.add(name, date, x, y, z)\n",
    "                fi.close()\n",
    "\n",
    "    # Store the data object for quicker loading later\n",
    "    with open(\"Korea_GPS_Stations.p\", \"wb\") as handle:\n",
    "        pickle.dump(Korea_GPS_Stations, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    DATA_EXISTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( Korea_GPS_Stations.data['PUSN'][-1,1] - Korea_GPS_Stations.data['PUSN'][0,1] )\n",
    "print( Korea_GPS_Stations.data['PUSN'][-1,0],Korea_GPS_Stations.data['PUSN'][0,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in Korea_GPS_Stations.data.keys():\n",
    "    cmd = \"wget \"\n",
    "    print(station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jump removal\n",
    "A function `remove_jump` is used for removing a jump in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_jump( stations, s, jump_date1, jump_date2 ):\n",
    "    d = stations.data[s][:,0]\n",
    "    e = stations.data[s][:,1]\n",
    "    n = stations.data[s][:,2]\n",
    "    u = stations.data[s][:,3]\n",
    "\n",
    "    jump_ind1 = np.where(d == jump_date1)[0]\n",
    "    jump_ind2 = np.where(d == jump_date2)[0]\n",
    "    after_jump = d > jump_date1\n",
    "    \n",
    "    #print(jump_ind1, jump_ind2)\n",
    "    #print(jump_ind2[0], jump_ind2[0]+5, stations.data[s][jump_ind2[0]:(jump_ind2[0]+5),1:4])\n",
    "    avg_period = 20\n",
    "    if s == 'JEOJ':\n",
    "        avg_period = 120\n",
    "    jumpval = np.sum(stations.data[s][jump_ind2[0]:(jump_ind2[0]+avg_period),1:4],\\\n",
    "                     axis=0)/avg_period\n",
    "    print(jumpval)\n",
    "    jump = jumpval - stations.data[s][jump_ind1,1:4]\n",
    "    #jump = stations.data[s][jump_ind2,1:4]-stations.data[s][jump_ind1,1:4]\n",
    "    #print(jump.shape)\n",
    "    new_e = np.where(after_jump, e-jump[0,0], e)\n",
    "    new_n = np.where(after_jump, n-jump[0,1], n)\n",
    "    new_u = np.where(after_jump, u-jump[0,2], u)\n",
    "    #plt.plot(d, new_u, color='red')\n",
    "    #plt.plot(d, u)\n",
    "\n",
    "    stations.data[s][:,1] = new_e\n",
    "    stations.data[s][:,2] = new_n\n",
    "    stations.data[s][:,3] = new_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stations with incomplete time period\n",
    "\n",
    "Most of the stations have a position time series spanning 2011/04/01 to 2016/12/31.\n",
    "Any station with a shorter data span is removed from the `Korea_GPS_Stations object`'s data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the loaded data if desired.\n",
    "stations_to_discard = []\n",
    "for s in Korea_GPS_Stations.data.keys():\n",
    "    if Korea_GPS_Stations.data[s][0,0] > datetime(2011,4,1).date() or \\\n",
    "        Korea_GPS_Stations.data[s][-1,0] < datetime(2016,12,31).date():\n",
    "        stations_to_discard.append(s)\n",
    "for s in stations_to_discard:\n",
    "    Korea_GPS_Stations.data.pop(s)\n",
    "    \n",
    "scounter = 0\n",
    "for s in Korea_GPS_Stations.data.keys():    \n",
    "    scounter += 1\n",
    "    print(scounter, s, Korea_GPS_Stations.data[s][0,0], Korea_GPS_Stations.data[s][-1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers with Extended Linear Trajectory Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_station_data()`\n",
    "A function that returns date, easting, northing and up for a `station` from `start_datetime` to `end_datetime`.\n",
    "\n",
    "#### `plot_selected_station()`\n",
    "A function that plots easting, northing and up for a `selected_station` from `start_datetime` to `end_datetime`.\n",
    "\n",
    "#### `eltm()` and `get_eltm()`\n",
    "Implementation of Extended Linear Trajectory Model (Bevis and Brown, J.Geodesy, 2014):\n",
    "\n",
    "$\n",
    "\\mathbf{x}(t) = \\sum_{i=1}^{np+1} \\mathbf{p}_{i}(t-t_{R})^{i-1} +\n",
    "\\sum_{j=1}^{nJ} \\mathbf{b}_{j}H(t-t_{j}) + \n",
    "\\sum_{k=1}^{nF} \\left( \\mathbf{s}_{k}\\sin(\\omega_{k}t) + \\mathbf{c}_{k}\\cos(\\omega_{k}t) \\right) +\n",
    "\\sum_{m=1}^{nT} \\mathbf{a}_{m}\\log(1+\\Delta t_{m}/T_{m}).\n",
    "$\n",
    "\n",
    "Here, we consider only the following model with 11 parameters:\n",
    "$\n",
    "\\mathbf{x}(t) = (pt+q) +\n",
    "\\left( \\mathbf{s}_{a}\\sin(\\omega_{a}t) + \\mathbf{c}_{a}\\cos(\\omega_{a}t) \\right) +\n",
    "\\left( \\mathbf{s}_{h}\\sin(\\omega_{h}t) + \\mathbf{c}_{h}\\cos(\\omega_{h}t) \\right) +\n",
    "a\\log(1+\\Delta t_{m}/T_{m}).\n",
    "$\n",
    "\n",
    "The difference between `eltm` and `get_eltm` is that `eltm` takes `days` as `float` while `get_eltm` internally converts `datetime` to `float`.\n",
    "\n",
    "#### `eltm_linear_only` and `get_eltm_linear_only()`\n",
    "Only the linear component is assumed to contribute to tectonic displacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_data( station, start_datetime=datetime(2011, 4, 1), end_datetime=datetime(2016, 12, 31) ):\n",
    "    \"\"\"\n",
    "    Retrieves a station's data from start_datetime to end_datetime (all inclusive)\n",
    "    and returns 4 arrays of daily time series, date, easting, northing and up.\n",
    "    Note that position components are shifted to be zero on the starting date.\n",
    "    \"\"\"\n",
    "    data = Korea_GPS_Stations.data[station]\n",
    "    start_date = start_datetime.date() \n",
    "    if start_date < data[0,0]:\n",
    "        start_date = data[0,0]\n",
    "        print(\"Warning: Start date reset to {0}\".format(start_date))\n",
    "    end_date = end_datetime.date()\n",
    "    if end_date > data[-1,0]:\n",
    "        end_date = data[-1,0]\n",
    "        print(\"Warning: End date reset to {0}\".format(end_date))\n",
    "    print(start_date, data[0,0], end_date, data[-1,0])\n",
    "    \n",
    "    datemask = np.logical_and(data[:,0] >= start_date, data[:,0] <= end_date)\n",
    "    start_index = np.where(data[:,0] == start_date)[0]\n",
    "    print(\"start_index = {}\".format(start_index))\n",
    "    if len(start_index) != 1:\n",
    "        print(\"start_index = \", start_index)\n",
    "        print(\"first and last date in the data are \", data[0,0], data[-1,0])\n",
    "    \n",
    "    date = data[datemask,0]\n",
    "    easting  = data[start_index,1] - data[datemask,   1]\n",
    "    northing = data[datemask,   2] - data[start_index,2]\n",
    "    up       = data[datemask,   3] - data[start_index,3]\n",
    "    print(data[start_index,2], data[start_index,3])\n",
    "    print(len(date), len(data[:,0]))\n",
    "    \n",
    "    return date, easting, northing, up\n",
    "\n",
    "def plot_selected_station( selected_station, start_datetime=datetime(2011, 9, 1), \n",
    "                          end_datetime=datetime(2016, 9, 30) ):\n",
    "    \"\"\"\n",
    "    plot the east, north and up position time series for a selected station \n",
    "    from a start date until an end date.\n",
    "    \"\"\"\n",
    "    d, e, n, u = get_station_data(selected_station, start_datetime, end_datetime)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    ax.plot(d, e, 'ro', markersize=2, label=\"east\")\n",
    "    ax.plot(d, n, 'go', markersize=2, label=\"north\")\n",
    "    ax.plot(d, u, 'bo', markersize=2, label=\"up\")\n",
    "    ax.set_title(selected_station)\n",
    "    ax.set_ylabel('Displacement (m)')\n",
    "    ax.set_ylim((-0.1,0.1))\n",
    "    ax.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eltm(days, p, q, sa, sp, ca, cp, sh, shp, ch, chp, a):\n",
    "    from numpy import sin, cos, log   \n",
    "    wa = 2.0*np.pi/365.0  # annual frequency\n",
    "    wh = 4.0*np.pi/365.0  # semi-annual frequency\n",
    "    # Number of days since Tohoku Eq (March 11, 2011) until April 1, 2011.\n",
    "    dd_eq = 21.0 # = (datetime(2011, 4, 1)-datetime(2011, 3, 11)) / timedelta(days=1)\n",
    "    dd = 2.0*(days + dd_eq)/365.0 # time scale: half a year\n",
    "    disp = (p*days+q) + sa * sin(wa*(days-sp)) + ca * cos(wa*(days-cp)) \\\n",
    "            + sh * sin(wh*(days-shp)) + ch * cos(wh*(days-chp)) + a * log(1.0+dd)\n",
    "    return disp\n",
    "\n",
    "def get_eltm(day, coeff):\n",
    "    days = day.astype(float)\n",
    "    p, q, sa, sp, ca, cp, sh, shp, ch, chp, a = coeff\n",
    "    from numpy import sin, cos, log   \n",
    "    wa = 2.0*np.pi/365.0  # annual frequency\n",
    "    wh = 4.0*np.pi/365.0  # semi-annual frequency\n",
    "    # Number of days since Tohoku Eq (March 11, 2011) until April 1, 2011.\n",
    "    dd_eq = 21.0 # = (datetime(2011, 4, 1)-datetime(2011, 3, 11)) / timedelta(days=1)\n",
    "    dd = 2.0*(days + dd_eq)/365.0 # time scale: half a year\n",
    "    disp = (p*days+q) + sa * sin(wa*(days-sp)) + ca * cos(wa*(days-cp)) \\\n",
    "            + sh * sin(wh*(days-shp)) + ch * cos(wh*(days-chp)) + a * log(1.0+dd)\n",
    "    return disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eltm_linear_only(days, p, q):\n",
    "    disp = p*days+q\n",
    "    return disp\n",
    "\n",
    "def get_eltm_linear_only(day, coeff):\n",
    "    days = day.astype(float)\n",
    "    p, q = coeff\n",
    "    disp = p*days+q\n",
    "    return disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_e_eltm( selected_station, d, e, n, u, e_eltm, cleaned_station_data, popt ):\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    for ax in axs:\n",
    "        for axis in ['top', 'bottom', 'left', 'right']:\n",
    "            ax.spines[axis].set_linewidth(2.0)\n",
    "    fig.set_figheight(4)\n",
    "    fig.set_figwidth(9)\n",
    "\n",
    "    axs[0].plot(d,e)\n",
    "    axs[0].tick_params(direction='out', length=6, width=2, labelsize=12)\n",
    "    axs[0].tick_params(which='minor', direction='out', length=4, width=2)\n",
    "    axs[0].set_title(\"{0}: Raw data\".format(selected_station), size=16)\n",
    "    axs[0].set_xlabel('Time', size=14)\n",
    "    axs[0].set_ylabel('Easting (m)', size=14)\n",
    "    axs[0].grid()\n",
    "\n",
    "    # Plot both data and the fitting function\n",
    "    from numpy import log\n",
    "    axs[1].plot(cleaned_station_data['days'], cleaned_station_data['e'],\\\n",
    "                marker='.', markeredgecolor='gray', markerfacecolor='gray', linestyle='none', alpha=1.0, label='Cleaned data')\n",
    "    axs[1].set_ylim((-0.05,0.2))\n",
    "    axs[1].plot(cleaned_station_data['days'], e_eltm, color='tab:red',\\\n",
    "            linewidth=2, label='ELTM')\n",
    "    axs[1].plot(cleaned_station_data['days'], popt[0]*cleaned_station_data['dfloat'] + popt[1], \\\n",
    "             color='tab:orange', linewidth=2, label='linear')\n",
    "    axs[1].plot(cleaned_station_data['days'], popt[0]*cleaned_station_data['dfloat'] + popt[1] \\\n",
    "             + popt[10]*log(1.0+2.0*(cleaned_station_data['dfloat'] + 20.0)/365.0),\\\n",
    "            color='tab:green', linewidth=2, label='linear+log')\n",
    "    axs[1].tick_params(direction='out', length=6, width=2, labelsize=12)\n",
    "    axs[1].tick_params(which='minor', direction='out', length=4, width=2)\n",
    "    axs[1].set_title(\"{0}: Cleaned data and ELTM\".format(selected_station), size=16)\n",
    "    axs[1].set_xlabel('Time', size=14)\n",
    "    axs[1].set_ylabel('Easting (m)', size=14)\n",
    "    legend = axs[1].legend(loc=4, fontsize=12)\n",
    "    legend.get_frame().set_linewidth(2)\n",
    "    axs[1].grid()\n",
    "\n",
    "    plt.savefig(\"./ELTM_plots/Cleaned_ELTM_{0}.png\".format(selected_station))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_e_eltm_linear_only( selected_station, d, e, n, u, e_eltm, \\\n",
    "                                cleaned_station_data, popt, cov, stage):\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    for ax in axs:\n",
    "        for axis in ['top', 'bottom', 'left', 'right']:\n",
    "            ax.spines[axis].set_linewidth(2.0)\n",
    "    fig.set_figheight(4)\n",
    "    fig.set_figwidth(9)\n",
    "\n",
    "    axs[0].plot(d,e)\n",
    "    axs[0].tick_params(direction='out', length=6, width=2, labelsize=12)\n",
    "    axs[0].tick_params(which='minor', direction='out', length=4, width=2)\n",
    "    if stage == 'A':\n",
    "        axs[0].set_xticks([datetime(2011,4,1),datetime(2012,4,1),datetime(2013,4,1)])\n",
    "    else:\n",
    "        axs[0].set_xticks([datetime(2013,4,1),datetime(2015,4,1),datetime(2016,12,31)])\n",
    "    axs[0].set_title(\"{0}: Raw data for Stage {1}\".format(selected_station, stage), size=16)\n",
    "    axs[0].set_xlabel('Time', size=12)\n",
    "    axs[0].set_ylabel('Easting (m)', size=14)\n",
    "    axs[0].grid()\n",
    "\n",
    "    # Plot both data and the fitting function\n",
    "    from numpy import log\n",
    "    axs[1].plot(cleaned_station_data['days'], cleaned_station_data['e'],\\\n",
    "                marker='.', markeredgecolor='gray', markerfacecolor='gray', linestyle='none', alpha=1.0, label='Cleaned data')\n",
    "    axs[1].set_ylim((-0.05,0.2))\n",
    "#     axs[1].plot(cleaned_station_data['days'], e_eltm, color='tab:red',\\\n",
    "#             linewidth=2, label='ELTM')\n",
    "    axs[1].plot(cleaned_station_data['days'], popt[0]*cleaned_station_data['dfloat'] + popt[1], \\\n",
    "             color='tab:orange', linewidth=2, label='linear fit')\n",
    "    axs[1].tick_params(direction='out', length=6, width=2, labelsize=12)\n",
    "    axs[1].tick_params(which='minor', direction='out', length=4, width=2)\n",
    "    if stage == 'A':\n",
    "        axs[1].set_xticks([datetime(2011,4,1),datetime(2012,4,1),datetime(2013,4,1)])\n",
    "    else:\n",
    "        axs[1].set_xticks([datetime(2013,4,1),datetime(2015,4,1),datetime(2016,12,31)])\n",
    "    axs[1].set_title(\"{0}: Cleaned data for Stage {1}\".format(selected_station, stage), size=16)\n",
    "    axs[1].set_xlabel('Time', size=14)\n",
    "    axs[1].set_ylabel('Easting (m)', size=14)\n",
    "    legend = axs[1].legend(loc=4, fontsize=12)\n",
    "    legend.get_frame().set_linewidth(2)\n",
    "    axs[1].grid()\n",
    "    \n",
    "    vel = popt[0]*365.0e3 # m/day to mm/yr\n",
    "    std = np.sqrt(np.diag(cov))[0]*365.0e3 # standard deviation in mm/yr\n",
    "    vel_text = \"Ve={0:.2f}\".format(vel)\n",
    "    std_text = \"{0:.2f}\".format(std)\n",
    "    if stage == 'A':\n",
    "        axs[1].text( datetime(2011,4,1), 0.175, vel_text+r'$\\pm$'+std_text+' mm/yr', size=16, color='red')\n",
    "    else:\n",
    "        axs[1].text( datetime(2013,4,1), 0.175, vel_text+r'$\\pm$'+std_text+' mm/yr', size=16, color='blue')\n",
    "\n",
    "    plt.savefig(\"./ELTM_plots/As_Kim_etal_{0}{1}.png\".format(selected_station, stage))\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function that computes ELTM coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eltm_as_Kim_etal_2018( selected_station, start_datetime, end_datetime ):\n",
    "    \"\"\"\n",
    "    1. Retrieve days, east, north and up time series at a selected station \n",
    "    using 'get_station_data()'; 2. remove outlier relative to the full ELTM\n",
    "    fitting; and 3. calculate coefficients for linear fitting to the cleaned\n",
    "    data.\n",
    "    Input: \n",
    "        - selected_station: A dictionary pairng selected station names\n",
    "                            with raw position time series\n",
    "        - start_datetime, end_datetime: start and end date of 'datetime' type\n",
    "    Output:\n",
    "        - ecoeff, ecov: ELTM coefficients and the associated covariance for\n",
    "                        east components\n",
    "        - ncoeff, ncov: for north components    \n",
    "    \"\"\"    \n",
    "    d, e, n, u = get_station_data(selected_station, start_datetime, end_datetime)\n",
    "\n",
    "    # Full ELTM fitting for outlier removal\n",
    "    from scipy.optimize import curve_fit\n",
    "    days_float = (d-start_datetime.date()) / timedelta(days=1)\n",
    "    ecoeff, ecov = curve_fit(eltm, days_float, e)\n",
    "    e_eltm = get_eltm(days_float, ecoeff)\n",
    "    e_detrended = e - e_eltm\n",
    "    ncoeff, ncov = curve_fit(eltm, days_float, n)\n",
    "    n_eltm = get_eltm(days_float, ncoeff)\n",
    "    n_detrended = n - n_eltm\n",
    "\n",
    "    # Outlier detect/removal\n",
    "    std_e = np.std(e_detrended)\n",
    "    max_deviations = 1.0\n",
    "    not_outlier = np.abs(e_detrended) < (max_deviations * std_e)\n",
    "\n",
    "    # Store cleaned station data for another ELTM fitting\n",
    "    cleaned_station_data = {}\n",
    "    cleaned_station_data['days'] = d[not_outlier]\n",
    "    cleaned_station_data['e'] = e[not_outlier]\n",
    "    cleaned_station_data['n'] = n[not_outlier]\n",
    "    cleaned_station_data['u'] = u[not_outlier]\n",
    "    cleaned_station_data['dfloat'] = days_float[not_outlier].astype('float64')\n",
    "\n",
    "    # Acquire ELTM again with the cleaned data\n",
    "    ecoeff, ecov = curve_fit(eltm_linear_only, cleaned_station_data['dfloat'], cleaned_station_data['e'])\n",
    "    e_eltm = get_eltm_linear_only(cleaned_station_data['dfloat'], ecoeff)\n",
    "    ncoeff, ncov = curve_fit(eltm_linear_only, cleaned_station_data['dfloat'], cleaned_station_data['n'])\n",
    "    n_eltm = get_eltm_linear_only(cleaned_station_data['dfloat'], ncoeff)\n",
    "    \n",
    "    stage = 'B'\n",
    "    if start_datetime < datetime(2013,1,1):\n",
    "        stage = 'A'\n",
    "    plot_raw_e_eltm_linear_only( selected_station, d, e, n, u, e_eltm, \\\n",
    "                                cleaned_station_data, ecoeff, ecov, stage)\n",
    "    \n",
    "    return ecoeff, ecov, ncoeff, ncov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station selector widget\n",
    "station_selector = widgets.Select(\n",
    "    options=Korea_GPS_Stations.data.keys(),\n",
    "    # NOTE (rclam): seems this has to be manually changed on my end when change GPS station\n",
    "    value='DAEJ', \n",
    "    rows=10,\n",
    "    description='Select a GPS station:',\n",
    "    disabled=False\n",
    ")\n",
    "display(station_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When processing stations one by one:\n",
    "selected_station = station_selector.value\n",
    "ecoeffA, ecovA, ncoeffA, ncovA = get_eltm_as_Kim_etal_2018( selected_station, datetime(2011,4,1), datetime(2013,3,11) )\n",
    "ecoeffB, ecovB, ncoeffB, ncovB = get_eltm_as_Kim_etal_2018( selected_station, datetime(2013,3,12), datetime(2016,12,31) )\n",
    "print(ecoeffA*365.0e3, ecoeffB*365.0e3)\n",
    "print(np.sqrt(np.diag(ecovA))*365.0e3, np.sqrt(np.diag(ecovB))*365.0e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data > 1 $\\sigma$ from the detrended filtered out.\n",
    "- Notes on 22 problematic stations\n",
    "    - 3 stations are retained after jump removed:\n",
    "        - HOMI, MARA, ULLE\n",
    "    - 19 stations discarded\n",
    "        - WPWN: Station unidentified and location unknown\n",
    "        - JEOJ: Jump removed unsatisfactorily.\n",
    "        - SOUL: Data corrupted after early 2015?\n",
    "        - TABK: Data corrupted after early 2012?\n",
    "        - DOKD: Data corrupted?\n",
    "        - SEJO: Data length shorter than others.\n",
    "        - GOJE: Data length shorter than others.\n",
    "        - KUSN: Data length shorter than others.\n",
    "        - YODK: Data length shorter than others.\n",
    "        - JIND: Data length shorter than others.\n",
    "        - YOIN: Data length shorter than others.\n",
    "        - SEJN: Data length shorter than others.\n",
    "        - GANH: Data length shorter than others.\n",
    "        - BONH: Data length shorter than others.\n",
    "        - DONH: Data length shorter than others.\n",
    "        - CHUL: Data length shorter than others.\n",
    "        - DANJ: Data length shorter than others.\n",
    "        - GOSG: Data length shorter than others.\n",
    "        - HCHN: Data length shorter than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Korea_GPS_Stations.data.keys()))\n",
    "problematic_stations = np.array(['JEOJ','SOUL','TABK','DOKD','SEJO','GOJE',\\\n",
    "                                 'KUSN','YODK','JIND','YOIN','SEJN','GANH',\\\n",
    "                                 'BONH','DONH','CHUL','DANJ','GOSG','HCHN','WPWN'])\n",
    "for station in problematic_stations:\n",
    "    try:\n",
    "        Korea_GPS_Stations.data.pop(station)\n",
    "        print(\"{0} removed\".format(station))\n",
    "    except KeyError:\n",
    "        print(\"{0} already removed\".format(station))\n",
    "        continue\n",
    "print(len(Korea_GPS_Stations.data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove jump in some stations\n",
    "remove_jump( Korea_GPS_Stations, 'HOMI', datetime(2013,8,4).date(), datetime(2013,8,5).date())\n",
    "#remove_jump( Korea_GPS_Stations, 'JEOJ', datetime(2013,8,1).date(), datetime(2013,8,3).date())\n",
    "remove_jump( Korea_GPS_Stations, 'MARA', datetime(2012,9,19).date(), datetime(2012,9,20).date())\n",
    "remove_jump( Korea_GPS_Stations, 'ULLE', datetime(2014,9,16).date(), datetime(2014,9,17).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When processing all the stations in a batch:\n",
    "counter = 0\n",
    "station_eltm_data_AB = {}\n",
    "for selected_station in Korea_GPS_Stations.data.keys():\n",
    "    ecoeffA, ecovA, ncoeffA, ncovA = get_eltm_as_Kim_etal_2018( selected_station, \\\n",
    "                                                    datetime(2011,4,1), datetime(2013,3,11) )\n",
    "    ecoeffB, ecovB, ncoeffB, ncovB = get_eltm_as_Kim_etal_2018( selected_station, \\\n",
    "                                                    datetime(2013,3,11), datetime(2016,12,31) )\n",
    "    counter = counter + 1\n",
    "    station_eltm_data_AB[selected_station] = {'ecoeffA':ecoeffA, 'ecovA':ecovA, \\\n",
    "                                              'ncoeffA':ncoeffA, 'ncovA':ncovA, \\\n",
    "                                              'ecoeffB':ecoeffB, 'ecovB':ecovB, \\\n",
    "                                              'ncoeffB':ncoeffB, 'ncovB':ncovB}\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set with the problematic stations removed is pickled as `Korea_GPS_Stations_20110401_20161231.p`.\n",
    "The linear and logarithmic coefficients and associated covariances from the ELTM fitting are saved as `Korea_GPS_Stations_coeff_cov_20110401_20161231.p`. Subsequent processing and analysis can start with loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stations that has data from 2011/04/11 to 2016/09/11.\n",
    "# with open('Korea_GPS_Stations_20110401_20161231.p', 'wb') as handle:\n",
    "#     pickle.dump(Korea_GPS_Stations, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('Korea_GPS_Stations_coeff_cov_AB.p', 'wb') as handle:\n",
    "    pickle.dump(station_eltm_data_AB, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
